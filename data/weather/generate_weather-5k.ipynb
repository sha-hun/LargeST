{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5675\n"
     ]
    }
   ],
   "source": [
    "import zipfile as zp\n",
    "import os\n",
    "df = zp.ZipFile('WEATHER-5K.zip')\n",
    "# for i in df.namelist():\n",
    "#     print(i)\n",
    "print(len(df.namelist()))\n",
    "# df.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEATHER-5K/global_weather_stations/01001099999.csv extracted\n",
      "WEATHER-5K/meta_info.json extracted\n",
      "WEATHER-5K/percentile.json extracted\n"
     ]
    }
   ],
   "source": [
    "files_to_extract = [\n",
    "    'WEATHER-5K/global_weather_stations/01001099999.csv',\n",
    "    'WEATHER-5K/meta_info.json',\n",
    "    'WEATHER-5K/percentile.json'\n",
    "]\n",
    "for file in files_to_extract:\n",
    "    df.extract(file)\n",
    "    print(f'{file} extracted')\n",
    "df.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 10)\n",
      "(0, 10)\n",
      "(0, 10)\n",
      "(0, 10)\n",
      "(0, 10)\n",
      "(0, 10)\n",
      "(0, 10)\n",
      "(0, 10)\n",
      "(0, 10)\n",
      "(0, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# 读取WEATHER-5K/global_weather_stations中以csv结尾的文件\n",
    "folder_path = \"WEATHER-5K/global_weather_stations\"\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "# mask代表数据的缺失情况\n",
    "for file in csv_files:\n",
    "    # 构造完整的文件路径\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    # 读取 CSV 文件\n",
    "    df = pd.read_csv(file_path)\n",
    "    # print(df.shape)\n",
    "    # print(type(df.iloc[1]['MASK']))\n",
    "    # print(type(df.iloc[1]['TIME_DIFF']))\n",
    "    # condition1 = (df['TIME_DIFF'] == 0) & (df['MASK'] != \"[1 1 1 1 1]\")\n",
    "    # condition2 = (df['TIME_DIFF'] == 999999.0) & (df['MASK'] != \"[0 0 0 0 0]\")\n",
    "    # print(df[condition2].shape)\n",
    "    # filtered_df = df[condition1 | condition2]\n",
    "    # print(filtered_df.index)\n",
    "    # print(df.head(5))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存储成npz文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_wind_rate 24.7\n",
      "original data shape: (8760, 10, 10)\n",
      "original columns: Index(['DATE', 'LONGITUDE', 'LATITUDE', 'TMP', 'DEW', 'WND_ANGLE', 'WND_RATE',\n",
      "       'SLP', 'MASK', 'TIME_DIFF'],\n",
      "      dtype='object')\n",
      "num_samples: 8760 num_nodes: 10\n",
      "idx min & max: 11 8748\n",
      "final data shape: (8760, 10, 6) idx shape: (8737,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "\n",
    "def generate_data_and_idx(data, x_offsets, y_offsets, add_time_of_day, add_day_of_week):\n",
    "    num_samples, num_nodes, input_dim = data.shape\n",
    "    print('num_samples:', num_samples, 'num_nodes:', num_nodes)\n",
    "\n",
    "    feature_list = [data]\n",
    "    \n",
    "    if add_time_of_day:\n",
    "        # 假设 data 的索引是时间戳，存储在第一个维度\n",
    "        time_ind = (np.arange(num_samples) - np.arange(num_samples) // 24) / 24.0\n",
    "        time_of_day = np.tile(time_ind, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
    "        feature_list.append(time_of_day)\n",
    "    \n",
    "    if add_day_of_week:\n",
    "        # 假设 data 的索引是时间戳，存储在第一个维度\n",
    "        dow = np.arange(num_samples) % 7\n",
    "        dow_tiled = np.tile(dow, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
    "        day_of_week = dow_tiled / 7.0\n",
    "        feature_list.append(day_of_week)\n",
    "\n",
    "    data = np.concatenate(feature_list, axis=-1)\n",
    "    \n",
    "    min_t = abs(min(x_offsets))\n",
    "    max_t = abs(num_samples - abs(max(y_offsets)))  # Exclusive\n",
    "    print('idx min & max:', min_t, max_t)\n",
    "    idx = np.arange(min_t, max_t, 1)\n",
    "    return data, idx\n",
    "\n",
    "\n",
    "folder_path = \"WEATHER-5K/global_weather_stations\"\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "def generate_train_val_test(args):\n",
    "    years = args['years'].split('_')\n",
    "    max_wind_rate = 0\n",
    "    for y in years:\n",
    "        year_data = []\n",
    "        for file in csv_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "            df = df[df['DATE'].dt.year == int(y)]\n",
    "            max_wind_rate = max(max_wind_rate, df['WND_RATE'].max())\n",
    "            df_value = df.values\n",
    "            year_data.append(df_value)\n",
    "\n",
    "        stacked_year_data = np.stack(year_data, axis=1)\n",
    "\n",
    "    print(\"max_wind_rate\",max_wind_rate)\n",
    "    print('original data shape:', stacked_year_data.shape)\n",
    "    print(\"original columns:\", df.columns)\n",
    "    seq_length_x, seq_length_y = args['seq_length_x'], args['seq_length_y']\n",
    "    x_offsets = np.arange(-(seq_length_x - 1), 1, 1)\n",
    "    y_offsets = np.arange(1, (seq_length_y + 1), 1)\n",
    "\n",
    "    data, idx = generate_data_and_idx(stacked_year_data, x_offsets, y_offsets, args['tod'], args['dow'])\n",
    "    \n",
    "    # 原本的列 ['DATE', 'LONGITUDE', 'LATITUDE', 'TMP', 'DEW', 'WND_ANGLE', 'WND_RATE','SLP', 'MASK', 'TIME_DIFF']\n",
    "    col_to_use = ['TMP', 'DEW', 'WND_ANGLE', 'WND_RATE', 'SLP', 'MASK']\n",
    "    data = data[:, :, [df.columns.get_loc(c) for c in col_to_use]]\n",
    "    # print(data)\n",
    "    print('final data shape:', data.shape, 'idx shape:', idx.shape)\n",
    "    \n",
    "    num_samples = len(idx)\n",
    "    num_train = round(num_samples * 0.6)\n",
    "    num_val = round(num_samples * 0.2)   \n",
    "\n",
    "    # split idx\n",
    "    idx_train = idx[:num_train]\n",
    "    idx_val = idx[num_train: num_train + num_val]\n",
    "    idx_test = idx[num_train + num_val:]\n",
    "    \n",
    "    #\n",
    "    normalize_col = ['TMP','DEW','SLP'] # 对应 0,1,4\n",
    "    max_normlize = ['WND_ANGLE','WND_RATE'] # 对应 2,3\n",
    "\n",
    "    # normalize\n",
    "    for row in [0,1,4]:\n",
    "        x_train = data[:idx_val[0] - args['seq_length_x'], :, row] \n",
    "        scaler = StandardScaler(mean=x_train.mean(), std=x_train.std())\n",
    "        data[..., row] = scaler.transform(data[..., row])\n",
    "\n",
    "    data[..., 2] = data[..., 2] / 360.0\n",
    "    data[..., 3] = data[..., 3] / max_wind_rate\n",
    "\n",
    "\n",
    "    # save\n",
    "    out_dir = args['dataset'] + '/' + args['years']\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    np.savez_compressed(os.path.join(out_dir, 'his.npz'), data=data, mean=scaler.mean, std=scaler.std)\n",
    "\n",
    "    np.save(os.path.join(out_dir, 'idx_train'), idx_train)\n",
    "    np.save(os.path.join(out_dir, 'idx_val'), idx_val)\n",
    "    np.save(os.path.join(out_dir, 'idx_test'), idx_test)\n",
    "\n",
    "arg = {\n",
    "    'dataset': 'weather',\n",
    "    'years': '2023',\n",
    "    'seq_length_x': 12,\n",
    "    'seq_length_y': 12,\n",
    "    'tod': 1,\n",
    "    'dow': 1\n",
    "}\n",
    "generate_train_val_test(arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays in the .npz file: ['data', 'mean', 'std']\n",
      "Data shape: (8760, 10, 6)\n",
      "-0.005590526966527427\n",
      "0.9704709305012484\n",
      "Mean shape: ()\n",
      "1008.6588885963583\n",
      "Std shape: ()\n",
      "12.399915678501442\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 读取 .npz 文件\n",
    "file_path = 'weather/2023/his.npz'  # 替换为你的 .npz 文件路径\n",
    "data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "# 查看 .npz 文件中包含的数组名称\n",
    "print(\"Arrays in the .npz file:\", data.files)\n",
    "\n",
    "\n",
    "in_data = data['data']\n",
    "in_mean = data['mean']\n",
    "in_std = data['std']\n",
    "\n",
    "print(\"Data shape:\", in_data.shape)\n",
    "\n",
    "print(\"Mean shape:\", in_mean.shape)\n",
    "print(in_mean)\n",
    "print(\"Std shape:\", in_std.shape)\n",
    "print(in_std)\n",
    "\n",
    "# 关闭文件（重要！）\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存储成H5文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "folder_path = \"WEATHER-5K/global_weather_stations\"\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "# mask代表数据的缺失情况\n",
    "year_data = []\n",
    "columns = ['DATE','LONGITUDE','LATITUDE','TMP','DEW','WND_ANGLE','WND_RATE','SLP','MASK','TIME_DIFF']\n",
    "columns_2 = ['DATE','LONGITUDE','LATITUDE','TMP','DEW','WND_ANGLE','WND_RATE','SLP','MASK_2','TIME_DIFF']\n",
    "def mask_to_binary(mask_str):\n",
    "    # 去掉首尾的括号\n",
    "    mask_str = mask_str.strip('[]')\n",
    "    # 将字符串分割成列表\n",
    "    mask_list = mask_str.split()\n",
    "    # 将列表中的字符串转换为整数\n",
    "    binary_numbers = [int(num) for num in mask_list]\n",
    "    # 将整数列表转换为二进制数\n",
    "    binary_str = ''.join(map(str, binary_numbers))\n",
    "    return int(binary_str, 2)\n",
    "\n",
    "for year in range(2014,3000):\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "        df = df[df['DATE'].dt.year == year]\n",
    "        df = df[columns]\n",
    "        df['MASK_2'] = df['MASK'].apply(mask_to_binary)\n",
    "        df = df[columns_2]\n",
    "\n",
    "        df_value = df.iloc[:,1:].values\n",
    "        \n",
    "        year_data.append(df_value)\n",
    "        # break\n",
    "    # print(df['DATE'].dt.year)\n",
    "    \n",
    "\n",
    "    stacked_year_data = np.stack(year_data, axis=1)\n",
    "    # print(stacked_year_data.shape)\n",
    "    # print(stacked_year_data[0][0])\n",
    "\n",
    "    # 创建一个名为 data 的数据集，数据集的内容就是我们的数据\n",
    "    file_path = f'weather_his_{year}.h5'\n",
    "    with h5py.File(file_path, 'a') as h5_file:\n",
    "        dataset_name = 'data'\n",
    "        \n",
    "        # 如果数据集存在，先删除它\n",
    "        if dataset_name in h5_file:\n",
    "            del h5_file[dataset_name]\n",
    "        \n",
    "        # 创建新的数据集\n",
    "        h5_file.create_dataset(dataset_name, data=stacked_year_data)\n",
    "        h5_file[dataset_name].attrs['columns'] = columns_2\n",
    "    break\n",
    "\n",
    "\n",
    "#   ca_his_2019.h5\n",
    "\n",
    "# print(df_year.shape)\n",
    "# print(df_year.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['data']>\n",
      "['DATE', 'LONGITUDE', 'LATITUDE', 'TMP', 'DEW', 'WND_ANGLE', 'WND_RATE', 'SLP', 'MASK_2', 'TIME_DIFF']\n",
      "数据形状: (8760, 10, 9)\n",
      "[-8.66666670e+00  7.09333333e+01 -8.00000000e-01 -4.00000000e+00\n",
      "  2.90000000e+02  6.00000000e+00  1.01220000e+03  3.10000000e+01\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'weather_his_2014.h5'\n",
    "\n",
    "# 打开 .h5 文件\n",
    "with h5py.File(file_path, 'r') as h5_file:\n",
    "    # 读取数据集\n",
    "    print(h5_file.keys())\n",
    "    print(list(h5_file[dataset_name].attrs['columns']))\n",
    "    dataset_name = 'data'  # 数据集的名称\n",
    "    if dataset_name in h5_file:\n",
    "        data = h5_file[dataset_name][:]\n",
    "        print(f\"数据形状: {data.shape}\")\n",
    "    else:\n",
    "        print(f\"数据集 '{dataset_name}' 不存在于文件中。\")\n",
    "    \n",
    "    # print(data.keys)\n",
    "    print(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "No conversion path for dtype: dtype('<M8[s]')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 77\u001b[0m\n\u001b[0;32m     71\u001b[0m structured_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m     72\u001b[0m     [\u001b[38;5;28mtuple\u001b[39m(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m stacked_data],\n\u001b[0;32m     73\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdt\n\u001b[0;32m     74\u001b[0m )\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# 创建数据集\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m hf\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweather_data\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mstructured_data)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# 添加元数据\u001b[39;00m\n\u001b[0;32m     80\u001b[0m hf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweather_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\dataset.py:86\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m---> 86\u001b[0m     tid \u001b[38;5;241m=\u001b[39m h5t\u001b[38;5;241m.\u001b[39mpy_create(dtype, logical\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Legacy\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[38;5;129;01mand\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1663\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1687\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1714\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1620\u001b[0m, in \u001b[0;36mh5py.h5t._c_compound\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1687\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1753\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: No conversion path for dtype: dtype('<M8[s]')"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm  # 可选进度条\n",
    "\n",
    "folder_path = \"WEATHER-5K/global_weather_stations\"\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "columns = ['DATE','LONGITUDE','LATITUDE','TMP','DEW','WND_ANGLE','WND_RATE','SLP','MASK','TIME_DIFF']\n",
    "\n",
    "# 修正年份范围（2014-3000不合理）\n",
    "start_year = 2014\n",
    "end_year = 2020  # 调整为实际需要的结束年份\n",
    "\n",
    "# 定义h5py变长字符串类型\n",
    "vlen_str = h5py.special_dtype(vlen=str)\n",
    "\n",
    "for year in tqdm(range(start_year, end_year+1)):\n",
    "    yearly_data = []\n",
    "    station_ids = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # 提取站点ID（假设文件名包含站点信息）\n",
    "            station_id = os.path.splitext(file)[0]\n",
    "            \n",
    "            # 转换日期并过滤年份\n",
    "            df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "            df_year = df[df['DATE'].dt.year == year]\n",
    "            \n",
    "            if not df_year.empty:\n",
    "                # 确保列存在\n",
    "                df_year = df_year[columns].copy()\n",
    "                \n",
    "                # 转换MASK列为字符串列表\n",
    "                df_year['MASK'] = df_year['MASK'].astype(str).tolist()\n",
    "                \n",
    "                # 存储站点ID和数据\n",
    "                station_ids.append(station_id)\n",
    "                yearly_data.append(df_year.values)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file} for year {year}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if yearly_data:\n",
    "        # 创建年度HDF5文件\n",
    "        with h5py.File(f'1weather_his_{year}.h5', 'w') as hf:\n",
    "            # 存储站点ID作为属性\n",
    "            hf.attrs['station_ids'] = station_ids\n",
    "            \n",
    "            # 创建复合数据类型\n",
    "            dt = np.dtype([\n",
    "                ('DATE', 'datetime64[s]'),\n",
    "                ('LONGITUDE', 'f4'),\n",
    "                ('LATITUDE', 'f4'),\n",
    "                ('TMP', 'f4'),\n",
    "                ('DEW', 'f4'),\n",
    "                ('WND_ANGLE', 'f4'),\n",
    "                ('WND_RATE', 'f4'),\n",
    "                ('SLP', 'f4'),\n",
    "                ('MASK', vlen_str),\n",
    "                ('TIME_DIFF', 'f4')\n",
    "            ])\n",
    "            \n",
    "            # 转换数据\n",
    "            stacked_data = np.concatenate(yearly_data)\n",
    "            structured_data = np.array(\n",
    "                [tuple(row) for row in stacked_data],\n",
    "                dtype=dt\n",
    "            )\n",
    "            \n",
    "            # 创建数据集\n",
    "            hf.create_dataset('weather_data', data=structured_data)\n",
    "            \n",
    "            # 添加元数据\n",
    "            hf['weather_data'].attrs['columns'] = columns\n",
    "            hf['weather_data'].attrs['year'] = year\n",
    "    else:\n",
    "        print(f\"No data found for year {year}\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
